{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression & Clustering #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression --> Pipeline ##\n",
    "We want to find a function f that that predicts gas consumption (y) based on weather data x1, x2, ..., xn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What type of regression?  Try using small sample and pick version that (a) minimizes error and (b) gives us small enough runtimes to be able to do it for each consumer ID:\n",
    "* Linear regression + LASSO\n",
    "* GLM/nonlinearn (polynomial)\n",
    "* GLM/nonlinear (poisson)\n",
    "* Kernel Ridge Regression (KRR)\n",
    "* SVR\n",
    "    * RBF\n",
    "    * Polynomial kernel\n",
    "\n",
    "How much data to use?  Last year?  All?  \n",
    "* What is runtime difference? \n",
    "* What is accuracy difference?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### parallel processing in python: \n",
    "\n",
    "* https://docs.python.org/3/library/multiprocessing.html \n",
    "* see also: Python Data Science Essentials ~pg390\n",
    "\n",
    "\n",
    "* http://docs.dask.org/en/latest/ \n",
    "* http://docs.dask.org/en/latest/scheduling.html \n",
    "* http://docs.dask.org/en/latest/setup/single-distributed.html \n",
    "* http://docs.dask.org/en/latest/use-cases.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1be70f9b4899435dbcda43d0a0ea2a9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h2>LocalCluster</h2>'), HBox(children=(HTML(value='\\n<div>\\n  <style scoped>\\n    â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "#from joblib import Parallel\n",
    "from dask import compute, delayed\n",
    "from dask.distributed import Client, LocalCluster \n",
    "cl = LocalCluster()\n",
    "client = Client(cl)\n",
    "cl\n",
    "# close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read pickle\n",
    "location = '/Users/mithras/Documents/_SCHOOL/_Drexel/BUSN 710 - Capstone/Data/Forecasting Project/'\n",
    "daily = pd.read_pickle(location+'peco_daily.pkl.zip')\n",
    "hourly = pd.read_pickle(location+'peco_hourly.pkl.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parallelize!  For each ID:\n",
    "\n",
    "# predict *gas* as a function of weather data\n",
    "    # how many lag-deltas for each weather variable? (hypothesis <= 3?)\n",
    "    # additive or multiplicative? (hypothesis: mult)\n",
    "    # degree polynomial (hypothesis = 3?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear+LASSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GLM poly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GLM poisson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KRR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVR RBF\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.grid_search import RandomizedSearchCV\n",
    "\n",
    "\n",
    "hypothesis = SVR(kernel='rbf', random_state=101)\n",
    "search_dict = {'C': [0.01, 0.1, 1, 10, 100], \n",
    "'gamma': [0.1, 0.01, 0.001, 0.0001]}\n",
    "search_func = RandomizedSearchCV(estimator=hypothesis, \n",
    "param_distributions=search_dict, n_iter=10, scoring='accuracy',\n",
    "n_jobs=-1, iid=True, refit=True, cv=5, random_state=101)\n",
    "search_func.fit(X_train, y_train)\n",
    "\n",
    "print ('Best parameters %s' % search_func.best_params_)\n",
    "print ('Cross validation accuracy: mean = %0.3f' % search_func.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVR poly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "def errfn (acts, preds, method):\n",
    "    \"\"\"\n",
    "    Calculates error using a variety of functions, \n",
    "    incl. Root Mean Squared Error (RMSE), Mean Absolute Error (MAE), & Log Accuracy Ratio (LNQ)\n",
    "    \n",
    "    LnQ acts as a symmetric percentage error function.  See: https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2635088\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    acts : actual values (ground truth) in list or numpy array\n",
    "    preds : predicted values  in list or numpy array\n",
    "    method : error function, including RMSE, MAE, LnQ\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    error : float\n",
    "\n",
    "    @author: Yi Zhu, Alex Graber\n",
    "    \"\"\"\n",
    "\n",
    "    # force method to uppercase\n",
    "    method = method.upper()\n",
    "    \n",
    "    # check to ensure equal length vectors\n",
    "    if (len(acts) != len(preds)):\n",
    "        print(\"Abs and Preds do not have equivalent length!\")\n",
    "    \n",
    "    # param detection\n",
    "    if (method == \"RMSE\"): \n",
    "        #calculate RMSE\n",
    "        from math import sqrt\n",
    "        from sklearn.metrics import mean_squared_error\n",
    "        error = sqrt(mean_squared_error(acts, preds))\n",
    "    elif (method == \"MAE\"):\n",
    "        #calculate MAE\n",
    "        from sklearn.metrics import mean_absolute_error\n",
    "        error = mean_absolute_error(acts, preds)\n",
    "    elif (method == \"LNQ\"):\n",
    "        #calculate LnQ\n",
    "        #numerator = abs(acts-preds)\n",
    "        #denominator = (abs(acts) + abs(preds))/2\n",
    "        #sum_n = numerator / denominator\n",
    "        error = np.sum(np.log(preds/acts)**2)\n",
    "    else:\n",
    "        print(\"Attempted method not in [RMSE, MAE, SMAPE]\")\n",
    "\n",
    "    return error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new df with ID, regression weights, regression error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method LocalCluster.close of LocalCluster('tcp://127.0.0.1:63002', workers=1, ncores=1)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# close dask cluster\n",
    "cl.close"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Sun Jun  3 20:17:26 2018\n",
    "\n",
    "@author: yizhu6\n",
    "\"\"\"\n",
    "\n",
    "# import W and reshape the data to get prepared, manually import\n",
    "nsamples, nx, ny = W_new.shape\n",
    "data_initial = W_new.reshape((nsamples,nx*ny))\n",
    "\n",
    "# remove zero obs\n",
    "df=pd.DataFrame(data_initial)\n",
    "df['total']= df.sum(axis=1)\n",
    "df_removezero = df[df.total != 0]\n",
    "data =np.array(df_removezero.drop(['total'],axis=1))\n",
    "\n",
    "#########################################\n",
    "\n",
    "# normalize\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "data_scaled = scaler.fit_transform(data)\n",
    "    \n",
    "# determine K using elbow method\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "from scipy.spatial.distance import cdist\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# euclidean\n",
    "start_time = time.time()\n",
    "sse1 = []\n",
    "K = range(1,12)\n",
    "for k in K:\n",
    "    kmeanModel = KMeans(n_clusters=k).fit(data)\n",
    "    kmeanModel.fit(data)\n",
    "    sse1.append(sum(np.min(cdist(data, kmeanModel.cluster_centers_, 'euclidean'), axis=1)) / X.shape[0])\n",
    "\n",
    "# Plot the elbow\n",
    "plt.plot(K, sse1, 'bx-')\n",
    "plt.xlabel('Number of K')\n",
    "plt.ylabel('SSE')\n",
    "plt.title('The Elbow Method Showing the Optimal K (Euclidean)')\n",
    "plt.show()\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "# do clustering\n",
    "import nltk\n",
    "from nltk.cluster.kmeans import KMeansClusterer\n",
    "from sklearn.cluster import AgglomerativeClustering,KMeans\n",
    "\n",
    "n_clusters = 5\n",
    "\n",
    "#############################################\n",
    "\n",
    "#KMeans - Euclidean\n",
    "kclusterer = KMeansClusterer(n_clusters, distance=nltk.cluster.util.euclidean_distance)\n",
    "clusters_table = kclusterer.cluster(data, assign_clusters=True)\n",
    "pd.DataFrame(pd.Series(clusters_table).value_counts(), columns = ['NO. of clients']).T\n",
    "#                  9   5   7   0   4   3   1   6   2   10  8 \n",
    "#NO. of clients  3908  13   6   5   4   3   3   3   3   2   1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for (k=2 to n):\n",
    "    # kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scree plot\n",
    "\n",
    "# super awesome visualization and screen plot kernels I came across while doing the NYC taxi fare forcasting project:\n",
    "# https://www.kaggle.com/ashishpatel26/exploration-of-nyc\n",
    "# https://www.kaggle.com/willkoehrsen/a-walkthrough-and-a-challenge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SSE \n",
    "from sklearn.cluster import KMeans\n",
    "sse = []\n",
    "K = range(1,20)\n",
    "for k in K:\n",
    "    kmeanModel = KMeans(n_clusters=k).fit(data)\n",
    "    kmeanModel.fit(data)\n",
    "    sse.append(sum(np.min(cdist(data, kmeanModel.cluster_centers_, 'cosine'), axis=1)) / X.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def: PCA\n",
    "    # normalize or not?\n",
    "\n",
    "pca_2cw = PCA(n_components=2, whiten=True)\n",
    "X_pca_1cw = pca_2cw.fit_transform(iris.data)\n",
    "plt.scatter(X_pca_1cw[:,0], X_pca_1cw[:,1], c=iris.target,\n",
    "alpha=0.8, s=60, marker='o', edgecolors='white')\n",
    "plt.show()\n",
    "pca_2cw.explained_variance_ratio_.sum() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize cluster identities using PCA on weight vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize rate-code identities using PCA on weight vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forecast ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autoARIMA (train, bounds):\n",
    "    \"\"\"\n",
    "    Runs an automatic ARIMA on data, given bounds to speed grid search for p,d,q,P,D,Q params\n",
    "    \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    train : training data\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    forecast : model\n",
    "\n",
    "    @author: Yi Zhu, Alex Graber\n",
    "    \"\"\"\n",
    "    \n",
    "    #preprocessing (since arima takes univariate series as input)\n",
    "    train.drop('Month',axis=1,inplace=True)\n",
    "    valid.drop('Month',axis=1,inplace=True)\n",
    "\n",
    "\n",
    "    #building the model\n",
    "    from pyramid.arima import auto_arima\n",
    "    model = auto_arima(train, trace=True, error_action='ignore', suppress_warnings=True)\n",
    "    model.fit(train)\n",
    "\n",
    "    forecast = model.predict(n_periods=len(valid))\n",
    "    forecast = pd.DataFrame(forecast,index = valid.index,columns=['Prediction'])\n",
    "\n",
    "    return forecast\n",
    "\n",
    "\n",
    "\n",
    "    # links\n",
    "    # https://www.analyticsvidhya.com/blog/2018/08/auto-arima-time-series-modeling-python-r/\n",
    "    # https://machinelearningmastery.com/arima-for-time-series-forecasting-with-python/\n",
    "    # https://medium.com/@josemarcialportilla/using-python-and-auto-arima-to-forecast-seasonal-time-series-90877adff03c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forecast each cluster\n",
    "fcasts = []\n",
    "for (i in 1:k):\n",
    "    # subset data based on cluster ID\n",
    "    \n",
    "    \n",
    "    # split data into training and testing\n",
    "    ratio = 0.7\n",
    "    train = data[:int(ratio*(len(data)))]\n",
    "    test = data[int(ratio*(len(data))):]\n",
    "\n",
    "    # forecast\n",
    "    fcast[i]=autoARIMA(train, bounds)\n",
    "    \n",
    "    # calculate error\n",
    "    preds = fcast[i].predictions\n",
    "    print(errfn(test, preds, \"RMSE\"))\n",
    "    print(errfn(test, preds, \"SMAPE\"))\n",
    "\n",
    "# compare forecast to testing data\n",
    "plt.plot(train, label='Train')\n",
    "plt.plot(test, label='test')\n",
    "plt.plot(forecast, label='Prediction')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forecast each rate code\n",
    "fcasts = []\n",
    "for (i in 1:k):\n",
    "    # subset data based on rate code\n",
    "    \n",
    "    \n",
    "    # split data into training and testing\n",
    "    ratio = 0.7\n",
    "    train = data[:int(ratio*(len(data)))]\n",
    "    test = data[int(ratio*(len(data))):]\n",
    "\n",
    "    # forecast\n",
    "    fcast[i]=autoARIMA(train, bounds)\n",
    "    \n",
    "    # calculate error\n",
    "    preds = fcast[i].predictions\n",
    "    print(errfn(test, preds, \"RMSE\"))\n",
    "    print(errfn(test, preds, \"SMAPE\"))\n",
    "\n",
    "# compare forecast to testing data\n",
    "plt.plot(train, label='Train')\n",
    "plt.plot(test, label='test')\n",
    "plt.plot(forecast, label='Prediction')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segmentation comparison ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* do any of our clusters correspond with rate code segments?\n",
    "* if so, can we correct rate codes based on segments directly?\n",
    "* if not, can we come up with a classifier to estimate rate code based on segment?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# identify nearest neighbors\n",
    "# KNN: K=10, default measure of distance (euclidean)\n",
    "clf = KNeighborsClassifier(3)\n",
    "clf.fit(X_train, y_train)\n",
    "preds = clf.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print (classification_report(acts, preds))\n",
    "\n",
    "# identify mode of neighbor classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Archive ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cross correlation ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The proper way to compare for relationships between time series is by the cross-correlation function (*assuming stationarity*). Having the same length is not essential. \n",
    "The cross correlation at lag 0 just computes a correlation like doing the Pearson correlation estimate pairing the data at the identical time points. If they do have the same length as you are assuming, you will have exact T pairs where T is the number of time points for each series. \n",
    "Lag 1 cross correlation matches time t from series 1 with time t+1 in series 2. Note that here even though the series are the same length you only have T-2 pair as one point in the first series has no match in the second and one other point in the second series will not have a match from the first. Given these two series you can estimate the cross-correlation at several lags . \n",
    "If any of the cross correlations is statistically significantly different from 0 it will indicate a correlation between the two series.\n",
    "see: https://stats.stackexchange.com/questions/29096/correlation-between-two-time-series, https://stats.stackexchange.com/questions/26842/correlating-volume-timeseries,\n",
    "https://stackoverflow.com/questions/33171413/cross-correlation-time-lag-correlation-with-pandas#37215839"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crosscorr(y, X, lag=0):\n",
    "    \"\"\" Lag-N cross correlation. \n",
    "    Parameters\n",
    "    ----------\n",
    "    lag : int, default 0\n",
    "    y : pandas.Series object; independent variable\n",
    "    X : pandas.Series object; matrix of dependent variables\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    crosscorr : float\n",
    "    \"\"\"\n",
    "    return X.corr(y.shift(lag))    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
